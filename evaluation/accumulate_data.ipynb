{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../exp/downloads/2024-02-14_11-37/11-37/'\n",
    "\n",
    "output_path = path\n",
    "\n",
    "csv_train_file_name = 'combined_train_data.csv'\n",
    "csv_eval_file_name = 'combined_eval_data.csv'\n",
    "\n",
    "agent_ids = ['follower0_0', 'follower1_0', 'follower2_0', 'follower3_0', 'follower4_0']\n",
    "summable_train_keys = ['actor_entropy', 'actor_loss', 'actor_target_entropy', 'alpha_loss', 'alpha_value', 'batch_reward', 'critic_loss', 'duration', 'episode_reward']\n",
    "static_train_keys = ['episode', 'step']\n",
    "summable_eval_keys = ['episode_reward']\n",
    "static_eval_keys = ['episode', 'step']\n",
    "\n",
    "train_file_names = []\n",
    "eval_file_names = []\n",
    "for id in agent_ids:\n",
    "    train_file_names.append(path + \"train_\" + id + \".csv\")\n",
    "    eval_file_names.append(path + \"eval_\" + id + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "'../exp/downloads/2024-02-17_11-32/11-32/',\n",
    "'../exp/downloads/2024-02-17_11-33/11-33/',\n",
    "'../exp/downloads/2024-02-17_11-37/11-37/',\n",
    "]\n",
    "\n",
    "csv_train_file_name = 'train_fed_0.33_4.csv'\n",
    "csv_eval_file_name = 'eval_fed_0.33_4.csv'\n",
    "\n",
    "output_path = '../exp/downloads/'\n",
    "\n",
    "summable_train_keys = ['actor_entropy', 'actor_loss', 'actor_target_entropy', 'alpha_loss', 'alpha_value', 'batch_reward', 'critic_loss', 'duration', 'episode_reward']\n",
    "static_train_keys = ['episode', 'step']\n",
    "summable_eval_keys = ['episode_reward']\n",
    "static_eval_keys = ['episode', 'step']\n",
    "\n",
    "train_file_names = []\n",
    "eval_file_names = []\n",
    "for path in paths:\n",
    "    train_file_names.append(path + \"combined_train_data.csv\")\n",
    "    eval_file_names.append(path + \"combined_eval_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine row number of smallest train file\n",
    "row_counts = []\n",
    "for file_index, file in enumerate(train_file_names):\n",
    "    df = pd.read_csv(file)\n",
    "    row_counts.append(len(df.index))\n",
    "min_row_count = min(row_counts)\n",
    "\n",
    "# prepare combined train file\n",
    "train_data_dict = {}\n",
    "for sum_key in summable_train_keys:\n",
    "    train_data_dict[sum_key] = []\n",
    "for static_key in static_train_keys:\n",
    "    train_data_dict[static_key] = []\n",
    "\n",
    "# aggregate train files\n",
    "for file_index, file in enumerate(train_file_names):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if index >= min_row_count:\n",
    "            break\n",
    "        \n",
    "        for sum_key in summable_train_keys:\n",
    "            if file_index == 0:\n",
    "                train_data_dict[sum_key].append(row[sum_key])\n",
    "            else:\n",
    "                train_data_dict[sum_key][index] += row[sum_key]\n",
    "        if file_index == 0:\n",
    "            for static_key in static_train_keys:\n",
    "                train_data_dict[static_key].append(row[static_key])\n",
    "\n",
    "for sum_key in summable_train_keys:\n",
    "    for row in range(len(train_data_dict[sum_key])):\n",
    "        train_data_dict[sum_key][row] /= len(train_file_names)\n",
    "\n",
    "\n",
    "# determine row number of smallest evaluation file\n",
    "row_counts = []\n",
    "for file_index, file in enumerate(eval_file_names):\n",
    "    df = pd.read_csv(file)\n",
    "    row_counts.append(len(df.index))\n",
    "min_row_count = min(row_counts)\n",
    "\n",
    "# prepare combined evaluation file\n",
    "eval_data_dict = {}\n",
    "for sum_key in summable_eval_keys:\n",
    "    eval_data_dict[sum_key] = []\n",
    "for static_key in static_eval_keys:\n",
    "    eval_data_dict[static_key] = []\n",
    "\n",
    "# aggregate evaluation files\n",
    "for file_index, file in enumerate(eval_file_names):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if index >= min_row_count:\n",
    "            break\n",
    "\n",
    "        for sum_key in summable_eval_keys:\n",
    "            if file_index == 0:\n",
    "                eval_data_dict[sum_key].append(row[sum_key])\n",
    "            else:\n",
    "                eval_data_dict[sum_key][index] += row[sum_key]\n",
    "        if file_index == 0:\n",
    "            for static_key in static_eval_keys:\n",
    "                eval_data_dict[static_key].append(row[static_key])\n",
    "\n",
    "for sum_key in summable_eval_keys:\n",
    "    for row in range(len(eval_data_dict[sum_key])):\n",
    "        eval_data_dict[sum_key][row] /= len(eval_file_names)\n",
    "\n",
    "\n",
    "# output new combined files\n",
    "train_df = pd.DataFrame.from_dict(train_data_dict)\n",
    "eval_df = pd.DataFrame.from_dict(eval_data_dict)\n",
    "\n",
    "train_df.to_csv(output_path + csv_train_file_name)\n",
    "eval_df.to_csv(output_path + csv_eval_file_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
